
Today the cost-efficient and stable supply of energy for production processes is a topic of increasing relevance. Changes in the structure of power generation towards decentralized systems and the growing share of renewable energy sources such as photovoltaics and wind power lead to a volatile situation in the supply of electrical energy. The concept in which electricity generation solely follows a demand based on statistical forecasts is no longer applicable. As a consequence, new concepts that harness the flexibility of energy consumption on the consumer side are becoming increasingly important, resulting in new developments in energy markets where a flexible consumer can get incentives for providing flexibility and participating in specific programs.

In this context, cyber-physical production systems (CPPS) with multiple possible plant topologies and the ability to produce huge number of product variants can make a significant contribution to a transition to a flexible energy demand. However, ensuring such flexibility in the industry 4.0 context requires a sophisticated control strategy to intelligently supervise the power consumption of the underlying components at every given time so that the total energy consumption follows given load profile boundaries. Moreover, the control strategy has to ensure a high productivity while coping with the uncertainties that can arise from the high fluctuations of weather-dependent energy prices, fast market changes as well as random unexpected disturbances in the factory floor.

Traditionally, analytical and simulation-based approaches are used to compute the optimal control strategy for the connected loads, distributed generation and storage components. However, these approaches often require complex models with precise descriptions of system features and functions. The engineering efforts for these systems are often a crucial aspect, which restricts implementations in the large scale. This problem becomes even more important in the industry 4.0 context, where fast market changes can lead to frequent changes in product and process configurations and therefore to the system models.

For these reasons, model-free control solutions and especially new artificial intelligence algorithms are considered as valuable alternatives or supplements \cite{Gholian7236921}. One of the most popular model-free control paradigms that has gained in importance in recent years is Reinforcement Learning (RL) \cite{Sutton9780262039246, bakakeu2018}: a model-free algorithm that does not require system identification steps or apriori knowledge. While being very popular in single agent settings (see \cite{mnih2013playing}, \cite{openai2019solving}, \cite{hausknecht2015deep}, \cite{schulman2017proximal}), a naive application of RL in a multi-agent environment  (as it is the case for CPPS) is likely to perform poorly. One issue is the non-Markovian nature of the resulting environment: since the action of one agent can affect the outcome of the action of other agents, the single perspective of an agent, which mainly consists of partial observations, becomes non-stationary in the sense that changes that cannot only be explained by the inherent stationary dynamics of the environment can occur. It is therefore important to coordinate the choices of actions of all agents in order to achieve the intended effects. Furthermore, an agent trained in a single environment setting is inherently bounded by the task description. A slight modification of the task or an evolution of the environment creates new pressures for retraining.

In order to solve these problems, many have worked on unsupervised exploration and skill acquisition methods such as \textit{intrinsic motivation} and \textit{self-play}. Rather than training the agents to solve a specific task in a predefined environment, the agent are conditioned in a autocurricula environment whereby the agents have to continuously coordinate their strategies (via competition or cooperation) in order to solve an ever evolving task. As a result, the agents learn an ensemble of emergent skills and coordination strategies rather than a specific complex policy making them more robust to environment changes.

There has been much success in leveraging multi-agent autocurricula to solve multi-agent problems such as \cite{lowe2017multiagent} and \cite{bansal2017emergent} both in discrete and continuous real-time domains. In both cases, self-play approaches provided the agents with a perfectly tuned curriculum. The success in these settings inspires confidence that inducing autocurricula in physically grounded and open-ended environments such industrial micro-grids could eventually enable the agents controlling the grid components to acquire an unbounded number of robust and emergent skills that can lead to a global resource optimization. In this chapter, we investigate whether the idea of autocurricula multi-agent reinforcement learning environments can yield fruit in solving the problem of the energy optimization of an heterogeneous cluster of cyber-physical production systems with energy generation and storage capabilities in an electricity micro-grid featuring a high volatility of electricity prices.

In more detail, we first formulate the energy optimization problem as an observable Markov game which we then use to design an autocurricula environment that simulates a micro-grid of flexible manufacturing machines with energy generation and energy storage capabilities. We then introduce several multi-agent tasks with comparative goals, where the different agents would need to learn not only highly developed skills but also coordinate the learned strategies in order to achieve the intended global effect. We then train the agents using a distributed implementation of a recent policy gradient algorithm, Proximal Policy Optimization (PPO) \cite{schulman2017proximal} in a framework of centralized training with decentralized execution. By adding a simple exploration curriculum to aid exploration in the environment, we found that in both cooperative and competitive scenarios different production machines were able to discover different coordination strategies in other to increase the energy efficiency of the whole factory floor.

The remainder of this chapter is structured as follows. In section \ref{sec:2}, backgrounds and related work on load management systems as well as multi-agent reinforcement learning algorithms are presented. Then, we formulate the energy optimization problem as a sequential decision making problem under uncertainty in section \ref{sec:3}. Section \ref{sec:4} gives a detailed description of our training algorithm and the autocurricula environment. Experimental results demonstrating the strength of our approach are presented in section \ref{sec:5}. Finally, we summarize the paper and discuss some directions for future work in section \ref{sec:6}.
