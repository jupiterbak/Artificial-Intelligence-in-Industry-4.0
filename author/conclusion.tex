This chapter presents an artificial intelligence algorithm especially a multi-agent reinforcement algorithm for the energy optimization of an heterogeneous cluster of flexible manufacturing machines with energy generation and energy storage capabilities in an electricity micro grid featuring a high volatility of electricity prices. It demonstrated that simple game rules, multi-agent competition, and standard reinforcement learning algorithms at scale can induce agents to learn complex coordination strategies and skills that can lead to an optimization of the global energy consumption while ensuring a high productivity. 

We analyze the performance of the proposed approach with respect to the number of agents and the micro-grid configuration in mixed cooperative and competitive scenarios and found that different production machines were able to discover different coordination strategies in other to increase the energy efficiency of the whole factory floor. These empirical results are promising and we intend to extend the solution to highly complicated and dynamic environments.

The presented results should be viewed as a proof of concept showing that multi-agent autocurricula can lead to physically grounded coordination strategies. We acknowledge that the strategy space in the proposed environments are inherently bounded and do not reflect the reality of the factory floor since many factors such as product quality, the production time and the unexpected human interventions in the production process have to be considered. However, because the algorithm was built on the functional abstraction of cyber-physical production systems, it is therefore applicable to real production systems if an invocation interface is connected to the agents. Our solution is therefore physically grounded and very extensible. In order to support further research, we are open-sourced our environment code \cite{Bakakeu2019}.
