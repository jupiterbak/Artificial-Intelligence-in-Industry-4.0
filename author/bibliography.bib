@book{Sutton9780262039246,
  Author = {Richard S. Sutton and Andrew G. Barto},
  Title = {Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning series)},
  Publisher = {A Bradford Book},
  Year = {2018},
  ISBN = {0262039249},
}

@misc{mnih2013playing,
    title={Playing Atari with Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2013},
    eprint={1312.5602},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{hausknecht2015deep,
    title={Deep Recurrent Q-Learning for Partially Observable MDPs},
    author={Matthew Hausknecht and Peter Stone},
    year={2015},
    eprint={1507.06527},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{openai2019solving,
    title={Solving Rubik's Cube with a Robot Hand},
    author={OpenAI and Ilge Akkaya and Marcin Andrychowicz and Maciek Chociej and Mateusz Litwin and Bob McGrew and Arthur Petron and Alex Paino and Matthias Plappert and Glenn Powell and Raphael Ribas and Jonas Schneider and Nikolas Tezak and Jerry Tworek and Peter Welinder and Lilian Weng and Qiming Yuan and Wojciech Zaremba and Lei Zhang},
    year={2019},
    eprint={1910.07113},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schulman2017proximal,
    title={Proximal Policy Optimization Algorithms},
    author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    year={2017},
    eprint={1707.06347},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lowe2017multiagent,
    title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
    author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
    year={2017},
    eprint={1706.02275},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{bansal2017emergent,
    title={Emergent Complexity via Multi-Agent Competition},
    author={Trapit Bansal and Jakub Pachocki and Szymon Sidor and Ilya Sutskever and Igor Mordatch},
    year={2017},
    eprint={1710.03748},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@ARTICLE{Gholian7236921,
author={A. {Gholian} and H. {Mohsenian-Rad} and Y. {Hua}},
journal={IEEE Transactions on Smart Grid},
title={Optimal Industrial Load Control in Smart Grid},
year={2016},
volume={7},
number={5},
pages={2305-2316},
keywords={energy management systems;energy storage;integer programming;linear programming;load regulation;optimal control;pricing;process control;smart power grids;optimal industrial load control;smart grid;industrial sector;residential appliances;batch cycles;process control;energy management;smart electricity pricing;peak pricing;time-of-use pricing;day-ahead pricing;block rates;renewable generator;energy storage;optimization problem;tractable mixed-integer linear program;energy-extensive steel mill industry model;Pricing;Load modeling;Load flow control;Industries;Batteries;Batch production systems;Job shop scheduling;Batch processes;demand side management;industrial load control (ILC);optimal energy scheduling;smart pricing},
doi={10.1109/TSG.2015.2468577},
ISSN={1949-3061},
month={Sep.},}

@article{CHEN2015263,
title = "Profit-seeking energy-intensive enterprises participating in power system scheduling: Model and mechanism",
journal = "Applied Energy",
volume = "158",
pages = "263 - 274",
year = "2015",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2015.08.018",
url = "http://www.sciencedirect.com/science/article/pii/S0306261915009447",
author = "Runze Chen and Hongbin Sun and Qinglai Guo and Hongyang Jin and Wenchuan Wu and Boming Zhang"
}

@INPROCEEDINGS{Wang6345296,
author={Z. {Wang} and F. {Gao} and Q. {Zhai} and X. {Guan} and K. {Liu} and D. {Zhou}},
booktitle={2012 IEEE Power and Energy Society General Meeting},
title={An integrated optimization model for generation and batch production load scheduling in energy intensive enterprise},
year={2012},
volume={},
number={},
pages={1-8},
doi={10.1109/PESGM.2012.6345296},
ISSN={1944-9925},
month={July},}

@article{MIDDELBERG20091266,
title = "An optimal control model for load shifting – With application in the energy management of a colliery",
journal = "Applied Energy",
volume = "86",
number = "7",
pages = "1266 - 1273",
year = "2009",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2008.09.011",
url = "http://www.sciencedirect.com/science/article/pii/S030626190800233X",
author = "Arno Middelberg and Jiangfeng Zhang and Xiaohua Xia",
}

@ARTICLE{Sun6329376,
author={Z. {Sun} and L. {Li}},
journal={IEEE Transactions on Automation Science and Engineering},
title={Opportunity Estimation for Real-Time Energy Control of Sustainable Manufacturing Systems},
year={2013},
volume={10},
number={1},
pages={38-44},
keywords={air pollution;energy conservation;manufacturing systems;sustainable development;opportunity estimation;realtime energy control;sustainable manufacturing system;energy efficiency;energy consumption;industrial environment;operation cost;company competitiveness;carbon dioxide emission;ecology;energy management;single machine system;sustainability point-of-view;stochastic factor;buffer utilization;multimachine manufacturing system;system throughput;automotive assembly line;Throughput;Manufacturing systems;Real-time systems;Carbon dioxide;Estimation;Maintenance engineering;Energy control;opportunity estimation;throughput constraint;sustainable manufacturing systems},
doi={10.1109/TASE.2012.2216876},
ISSN={1558-3783},
month={Jan},}

@article{DUFLOU2012587,
title = "Towards energy and resource efficient manufacturing: A processes and systems approach",
journal = "CIRP Annals",
volume = "61",
number = "2",
pages = "587 - 609",
year = "2012",
issn = "0007-8506",
doi = "https://doi.org/10.1016/j.cirp.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0007850612002016",
author = "Joost R. Duflou and John W. Sutherland and David Dornfeld and Christoph Herrmann and Jack Jeswiet and Sami Kara and Michael Hauschild and Karel Kellens",
keywords = "Energy efficiency, Manufacturing, Sustainable development",
abstract = "This paper aims to provide a systematic overview of the state of the art in energy and resource efficiency increasing methods and techniques in the domain of discrete part manufacturing, with attention for the effectiveness of the available options. For this purpose a structured approach, distinguishing different system scale levels, is applied: starting from a unit process focus, respectively the multi-machine, factory, multi-facility and supply chain levels are covered. Determined by the research contributions reported in literature, the de facto focus of the paper is mainly on energy related aspects of manufacturing. Significant opportunities for systematic efficiency improving measures are identified and summarized in this area."
}

@InProceedings{Pechmann2011,
author="Pechmann, Agnes
and Sch{\"o}ler, Ilka",
editor="Hesselbach, J{\"u}rgen
and Herrmann, Christoph",
title="Optimizing Energy Costs by Intelligent Production Scheduling",
booktitle="Glocalized Solutions for Sustainability in Manufacturing",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="293--298",
abstract="Small and medium sized enterprises tackle the problem of high energy consumption and costs. Reducing both by fixing leaks, improving the thermal insulation of buildings and switching of unnecessary heating/cooling are first steps. More advanced steps are reengineering machinery and processes. Within this paper results of a research project are presented. Unused potential for increasing energy efficiency can be found by optimizing the production schedule. A full scale PPC software was developed which is able to schedule production without energy peaks and furthermore is able to create a 24h energy forecast to hand over to the energy supplier.",
isbn="978-3-642-19692-8"
}

@article{BRUZZONE2012459,
title = "Energy-aware scheduling for improving manufacturing process sustainability: A mathematical model for flexible flow shops",
journal = "CIRP Annals",
volume = "61",
number = "1",
pages = "459 - 462",
year = "2012",
issn = "0007-8506",
doi = "https://doi.org/10.1016/j.cirp.2012.03.084",
url = "http://www.sciencedirect.com/science/article/pii/S0007850612000868",
author = "A.A.G. Bruzzone and D. Anghinolfi and M. Paolucci and F. Tonelli",
keywords = "Energy, Scheduling, Planning",
abstract = "Energy-aware scheduling (EAS) of manufacturing processes demands a mathematical model to optimally plan energy saving for a given schedule. The proposed approach starts from a reference schedule generated by an advanced planning and scheduling (APS) system which does not consider energy saving. The new approach relies on a mixed integer programming (MIP) model where the reference schedule is modified to account for energy consumption without changing the jobs’ assignment and sequencing provided by the reference schedule. The applicability of the approach has been validated through a test case; the results obtained using one commercial MIP solver and an original MIP-heuristic are discussed."
}

@article{FANG2011234,
title = "A new approach to scheduling in manufacturing for power consumption and carbon footprint reduction",
journal = "Journal of Manufacturing Systems",
volume = "30",
number = "4",
pages = "234 - 240",
year = "2011",
note = "Selected Papers of 39th North American Manufacturing Research Conference",
issn = "0278-6125",
doi = "https://doi.org/10.1016/j.jmsy.2011.08.004",
url = "http://www.sciencedirect.com/science/article/pii/S0278612511000690",
author = "Kan Fang and Nelson Uhan and Fu Zhao and John W. Sutherland",
keywords = "Scheduling, Peak load, Carbon footprint, Makespan",
abstract = "Manufacturing scheduling strategies have historically emphasized cycle time; in almost all cases, energy and environmental factors have not been considered in scheduling. This paper presents a new mathematical programming model of the flow shop scheduling problem that considers peak power load, energy consumption, and associated carbon footprint in addition to cycle time. The new model is demonstrated using a simple case study: a flow shop where two machines are employed to produce a variety of parts. In addition to the processing order of the jobs, the proposed scheduling problem considers the operation speed as an independent variable, which can be changed to affect the peak load and energy consumption. Even with a single objective, finding an optimal schedule is notoriously difficult, so directly applying commercial software to this multi-objective scheduling problem requires significant computation time. This paper calls for the development of more specialized algorithms for this new scheduling problem and examines computationally tractable approaches for finding near-optimal schedules."
}

@ARTICLE{Liu6918520,
author={C. {Liu} and X. {Xu} and D. {Hu}},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Multiobjective Reinforcement Learning: A Comprehensive Overview},
year={2015},
volume={45},
number={3},
pages={385-398},
keywords={decision making;learning (artificial intelligence);multi-agent systems;optimisation;multiobjective reinforcement learning;sequential decision-making;RL algorithms;MORL;multiobjective optimization;hierarchical RL;multiagent RL;Decision making;Approximation algorithms;Linear programming;Approximation methods;Optimization;Equations;Vectors;Markov decision process (MDP);multiobjective reinforcement learning (MORL);Pareto front;reinforcement learning (RL);sequential decision-making;Markov decision process (MDP);multiobjective reinforcement learning (MORL);Pareto front;reinforcement learning (RL);sequential decision-making},
doi={10.1109/TSMC.2014.2358639},
ISSN={2168-2232},
month={March},}

@article{ANDERSON1997421,
title = "Synthesis of reinforcement learning, neural networks and PI control applied to a simulated heating coil",
journal = "Artificial Intelligence in Engineering",
volume = "11",
number = "4",
pages = "421 - 429",
year = "1997",
note = "Applications of Neural Networks in Process Engineering",
issn = "0954-1810",
doi = "https://doi.org/10.1016/S0954-1810(97)00004-6",
url = "http://www.sciencedirect.com/science/article/pii/S0954181097000046",
author = "Charles W. Anderson and Douglas C. Hittle and Alon D. Katz and R.Matt Kretchmar",
keywords = "neural networks, reinforcement learning, PI control, HVAC, heating coil",
abstract = "An accurate simulation of a heating coil is used to compare the performance of a proportional plus integral (PI) controller to the following schemes for learning improved control: a neural network trained to predict the steady-state output of the PI controller, a neutral network trained to minimize the n-step ahead error between the coil output and the set point, and a reinforcement learning agent trained to minimize the sum of the squared error over time. Although the PI controller works very well for this task, the neural networks showed improved performance. The reinforcement learning agent, when combined with a PI controller, learned to augment the PI control output for the subset of states for which control can be improved."
}

@ARTICLE{Li6519950,
author={L. {Li} and Z. {Sun}},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Dynamic Energy Control for Energy Efficiency Improvement of Sustainable Manufacturing Systems Using Markov Decision Process},
year={2013},
volume={43},
number={5},
pages={1195-1205},
keywords={approximation theory;assembling;energy consumption;global warming;manufacturing systems;Markov processes;sustainable development;dynamic energy control;energy efficiency improvement;sustainable manufacturing systems;Markov decision process;greenhouse gas emissions;global warming;real time systematic management method;energy consumption;production constraints;approximate algorithm;single machine manufacturing systems;production buffers;assembly line;Energy efficiency improvement;Markov decision process (MDP);sustainable manufacturing systems},
doi={10.1109/TSMC.2013.2256856},
ISSN={2168-2232},
month={Sep.},}

@INPROCEEDINGS{Ruelens7038106,
author={F. {Ruelens} and B. J. {Claessens} and S. {Vandael} and S. {Iacovella} and P. {Vingerhoets} and R. {Belmans}},
booktitle={2014 Power Systems Computation Conference},
title={Demand response of a heterogeneous cluster of electric water heaters using batch reinforcement learning},
year={2014},
volume={},
number={},
pages={1-7},
keywords={demand side management;electric heating;intelligent control;learning (artificial intelligence);optimal control;power grids;power system control;thermal energy storage;water supply;market based heuristic;water tank;thermal storage;minimum electricity cost;cluster scheduling;domestic electric water heater;model-free control;system dynamics;optimal control;batch reinforcement learning;heterogeneous cluster;demand response;Water heating;Resistance heating;Trajectory;Electricity;Load modeling;Temperature sensors;Learning (artificial intelligence);Aggregator;demand response;batch reinforcement learning;electric water heater;fitted Q-iteration},
doi={10.1109/PSCC.2014.7038106},
ISSN={null},
month={Aug},}

@ARTICLE{Ruelens7792709,
author={F. {Ruelens} and B. J. {Claessens} and S. {Quaiyum} and B. {De Schutter} and R. {Babuška} and R. {Belmans}},
journal={IEEE Transactions on Smart Grid},
title={Reinforcement Learning Applied to an Electric Water Heater: From Theory to Practice},
year={2018},
volume={9},
number={4},
pages={3792-3800},
keywords={decision making;learning (artificial intelligence);Markov processes;temperature sensors;thermostats;sequential decision-making problem;Markov decision process;auto-encoder network;sensor measurements;batch reinforcement learning technique;fitted Q-iteration;control policy;temperature sensors;electric water heater;Water heating;Resistance heating;Load management;Learning (artificial intelligence);Temperature measurement;Temperature sensors;Energy consumption;Auto-encoder network;demand response;electric water heater;fitted Q-iteration;machine learning;reinforcement learning},
doi={10.1109/TSG.2016.2640184},
ISSN={1949-3061},
month={July},}

@INPROCEEDINGS{Somer8260152,
author={O. {De Somer} and A. {Soares} and K. {Vanthournout} and F. {Spiessens} and T. {Kuijpers} and K. {Vossen}},
booktitle={2017 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)},
title={Using reinforcement learning for demand response of domestic hot water buffers: A real-life demonstration},
year={2017},
volume={},
number={},
pages={1-7},
keywords={building management systems;decision making;demand side management;learning (artificial intelligence);photovoltaic power systems;power generation scheduling;space heating;thermostats;water supply;demand response;domestic hot water buffers;heating cycles;reinforcement learning technique;PV production;residential buildings;photovoltaic production;sequential decision-making problem;thermostat control;Temperature measurement;Temperature sensors;Water heating;Space heating;Heat pumps;Boilers;Demand Response;Domestic Hot Water;Field Experiment;Reinforcement Learning},
doi={10.1109/ISGTEurope.2017.8260152},
ISSN={null},
month={Sep.},}

@article{KAZMI2018159,
title = "Gigawatt-hour scale savings on a budget of zero: Deep reinforcement learning based optimal control of hot water systems",
journal = "Energy",
volume = "144",
pages = "159 - 168",
year = "2018",
issn = "0360-5442",
doi = "https://doi.org/10.1016/j.energy.2017.12.019",
url = "http://www.sciencedirect.com/science/article/pii/S0360544217320388",
author = "Hussain Kazmi and Fahad Mehmood and Stefan Lodeweyckx and Johan Driesen",
keywords = "Deep reinforcement learning, Domestic hot water, Optimal control, Energy efficiency, Smart grid"
}

@ARTICLE{Jiang6912013,
author={B. {Jiang} and Y. {Fei}},
journal={IEEE Transactions on Smart Grid},
title={Smart Home in Smart Microgrid: A Cost-Effective Energy Ecosystem With Intelligent Hierarchical Agents},
year={2015},
volume={6},
number={1},
pages={3-13},
keywords={cogeneration;cost reduction;distributed power generation;ecology;flow batteries;home automation;power consumption;power generation economics;power system management;smart power grids;smart home;cost-effective energy ecosystem;intelligent hierarchical agent;power generation efficiency;distributed renewable energy generation;cost-effective smart microgrid;dynamic demand response;DR;distributed energy resource management;DER management;dynamic update mechanism;micro combined heat and power system;μCHP;vanadium redox battery;VRB;energy consumption cost reduction;Optimization;Cogeneration;Communities;Wind power generation;Microgrids;Resistance heating;Fuels;Demand response (DR);distributed energy resources (DER);microgrid;particle swarm optimization (PSO);Q-learning;smart grid;Demand response (DR);distributed energy resources (DER);microgrid;particle swarm optimization (PSO);Q-learning;smart grid},
doi={10.1109/TSG.2014.2347043},
ISSN={1949-3061},
month={Jan},}

@article{ANVARIMOGHADDAM201741,
title = "A multi-agent based energy management solution for integrated buildings and microgrid system",
journal = "Applied Energy",
volume = "203",
pages = "41 - 56",
year = "2017",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2017.06.007",
url = "http://www.sciencedirect.com/science/article/pii/S0306261917307572",
author = "Amjad Anvari-Moghaddam and Ashkan Rahimi-Kian and Maryam S. Mirian and Josep M. Guerrero",
keywords = "Energy management, Distributed generation, Multi-agent systems (MAS), Optimization, Residential microgrid",
abstract = "In this paper, an ontology-driven multi-agent based energy management system (EMS) is proposed for monitoring and optimal control of an integrated homes/buildings and microgrid system with various renewable energy resources (RESs) and controllable loads. Different agents ranging from simple-reflex to complex learning agents are designed and implemented to cooperate with each other to reach an optimal operating strategy for the mentioned integrated energy system (IES) while meeting the system’s objectives and related constraints. The optimization process for the EMS is defined as a coordinated distributed generation (DG) and demand response (DR) management problem within the studied environment and is solved by the proposed agent-based approach utilizing cooperation and communication among decision agents. To verify the effectiveness and applicability of the proposed multi-agent based EMS, several case studies are carried out and corresponding results are presented."
}

@misc{prasad2018multiagent,
    title={Multi-agent Deep Reinforcement Learning for Zero Energy Communities},
    author={Amit Prasad and Ivana Dusparic},
    year={2018},
    eprint={1810.03679},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{LU2018220,
title = "A Dynamic pricing demand response algorithm for smart grid: Reinforcement learning approach",
journal = "Applied Energy",
volume = "220",
pages = "220 - 230",
year = "2018",
issn = "0306-2619",
doi = "https://doi.org/10.1016/j.apenergy.2018.03.072",
url = "http://www.sciencedirect.com/science/article/pii/S0306261918304112",
author = "Renzhi Lu and Seung Ho Hong and Xiongfeng Zhang",
keywords = "Demand response, Dynamic pricing, Artificial intelligence, Reinforcement learning, Markov decision process, Q-learning",
abstract = "With the modern advanced information and communication technologies in smart grid systems, demand response (DR) has become an effective method for improving grid reliability and reducing energy costs due to the ability to react quickly to supply-demand mismatches by adjusting flexible loads on the demand side. This paper proposes a dynamic pricing DR algorithm for energy management in a hierarchical electricity market that considers both service provider’s (SP) profit and customers’ (CUs) costs. Reinforcement learning (RL) is used to illustrate the hierarchical decision-making framework, in which the dynamic pricing problem is formulated as a discrete finite Markov decision process (MDP), and Q-learning is adopted to solve this decision-making problem. Using RL, the SP can adaptively decide the retail electricity price during the on-line learning process where the uncertainty of CUs’ load demand profiles and the flexibility of wholesale electricity prices are addressed. Simulation results show that this proposed DR algorithm, can promote SP profitability, reduce energy costs for CUs, balance energy supply and demand in the electricity market, and improve the reliability of electric power systems, which can be regarded as a win-win strategy for both SP and CUs."
}

@inproceedings{Littman1994multiagent,
    title={Markov games as a framework for multi-agent reinforcement learning},
    author={Michael L. Littman},
    year={1994},
    booktitle={ICML},
    pages = "157–163",
}

@inproceedings{Hu1998MultiagentRL,
  title={Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm},
  author={Junling Hu and Michael P. Wellman},
  booktitle={ICML},
  year={1998}
}

@ARTICLE{Busoniu4445757,
author={L. {Busoniu} and R. {Babuska} and B. {De Schutter}},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Comprehensive Survey of Multiagent Reinforcement Learning},
year={2008},
volume={38},
number={2},
pages={156-172},
keywords={learning (artificial intelligence);multi-agent systems;multiagent reinforcement learning;multiagent system;distributed control;game theory;Learning;Multiagent systems;Robots;Control systems;Resource management;Marine technology;Feedback;Distributed control;Environmental economics;Mechanical engineering;Distributed control;game theory;multiagent systems;reinforcement learning;Distributed control;game theory;multiagent systems;reinforcement learning},
doi={10.1109/TSMCC.2007.913919},
ISSN={1558-2442},
month={March},}

@INPROCEEDINGS{Tan93multiagentreinforcement,
    author = {Ming Tan},
    title = {Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents},
    booktitle = {In Proceedings of the Tenth International Conference on Machine Learning},
    year = {1993},
    pages = {330--337},
    publisher = {Morgan Kaufmann}
}

@article{Matignon2012IndependentRL,
  title={Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems},
  author={La{\"e}titia Matignon and Guillaume J. Laurent and Nadine Le Fort-Piat},
  journal={Knowledge Eng. Review},
  year={2012},
  volume={27},
  pages={1-31}
}

@misc{tampuu2015multiagent,
    title={Multiagent Cooperation and Competition with Deep Reinforcement Learning},
    author={Ardi Tampuu and Tambet Matiisen and Dorian Kodelja and Ilya Kuzovkin and Kristjan Korjus and Juhan Aru and Jaan Aru and Raul Vicente},
    year={2015},
    eprint={1511.08779},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@InProceedings{Gupta71682,
author="Gupta, Jayesh K.
and Egorov, Maxim
and Kochenderfer, Mykel",
editor="Sukthankar, Gita
and Rodriguez-Aguilar, Juan A.",
title="Cooperative Multi-agent Control Using Deep Reinforcement Learning",
booktitle="Autonomous Agents and Multiagent Systems",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="66--83",
abstract="This work considers the problem of learning cooperative policies in complex, partially observable domains without explicit communication. We extend three classes of single-agent deep reinforcement learning algorithms based on policy gradient, temporal-difference error, and actor-critic methods to cooperative multi-agent systems. To effectively scale these algorithms beyond a trivial number of agents, we combine them with a multi-agent variant of curriculum learning. The algorithms are benchmarked on a suite of cooperative control tasks, including tasks with discrete and continuous actions, as well as tasks with dozens of cooperating agents. We report the performance of the algorithms using different neural architectures, training procedures, and reward structures. We show that policy gradient methods tend to outperform both temporal-difference and actor-critic methods and that curriculum learning is vital to scaling reinforcement learning algorithms in complex multi-agent domains.",
isbn="978-3-319-71682-4"
}

@misc{foerster2017stabilising,
    title={Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning},
    author={Jakob Foerster and Nantas Nardelli and Gregory Farquhar and Triantafyllos Afouras and Philip H. S. Torr and Pushmeet Kohli and Shimon Whiteson},
    year={2017},
    eprint={1702.08887},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{Lowe2017MultiAgentAF,
  title={Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  author={Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.02275}
}

@inproceedings{Foerster2017CounterfactualMP,
  title={Counterfactual Multi-Agent Policy Gradients},
  author={Jakob N. Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
  booktitle={AAAI},
  year={2017}
}

@incollection{NIPS2016_6398,
title = {Learning Multiagent Communication with Backpropagation},
author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2244--2252},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf}
}

@article{MordatchA17,
  author    = {Igor Mordatch and
               Pieter Abbeel},
  title     = {Emergence of Grounded Compositional Language in Multi-Agent Populations},
  journal   = {CoRR},
  volume    = {abs/1703.04908},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04908},
  archivePrefix = {arXiv},
  eprint    = {1703.04908},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MordatchA17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{FoersterAFW16a,
  author    = {Jakob N. Foerster and
               Yannis M. Assael and
               Nando de Freitas and
               Shimon Whiteson},
  title     = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1605.06676},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.06676},
  archivePrefix = {arXiv},
  eprint    = {1605.06676},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/FoersterAFW16a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rashid2018,
  author    = {Tabish Rashid and
               Mikayel Samvelyan and
               Christian Schr{\"{o}}der de Witt and
               Gregory Farquhar and
               Jakob N. Foerster and
               Shimon Whiteson},
  title     = {{QMIX:} Monotonic Value Function Factorisation for Deep Multi-Agent
               Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1803.11485},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.11485},
  archivePrefix = {arXiv},
  eprint    = {1803.11485},
  timestamp = {Mon, 13 Aug 2018 16:46:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-11485},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

  @inproceedings{Sunehag3238080,
 author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and et al.},
 title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
 year = {2018},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
 pages = {2085–2087},
 numpages = {3},
 keywords = {collaborative, neural networks, multi-agent, value-decomposition, reinforcement learning, dqn, q-learning},
 location = {Stockholm, Sweden},
 series = {AAMAS ’18}
}

 @inproceedings{He3045581,
 author = {He, He and Boyd-Graber, Jordan and Kwok, Kevin and Daum\'{e}, Hal},
 title = {Opponent Modeling in Deep Reinforcement Learning},
 year = {2016},
 publisher = {JMLR.org},
 booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
 pages = {1804–1813},
 numpages = {10},
 location = {New York, NY, USA},
 series = {ICML’16}
}

@article{Choi2017MultifocusAN,
  title={Multi-focus Attention Network for Efficient Deep Reinforcement Learning},
  author={Jinyoung Choi and Beom-Jin Lee and Byoung-Tak Zhang},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.04603}
}

@inproceedings{Jiang3327828,
author = {Jiang, Jiechuan and Lu, Zongqing},
title = {Learning Attentional Communication for Multi-Agent Cooperation},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7265–7275},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS’18}
}

@misc{Brockman01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540}
}

@misc{Bakakeu2019,
  author = {Bakakeu, Jupiter},
  title = {Multi-Agent-RL-Energy-Management},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jupiterbak/Multi-Agent-RL-Energy-Management}},
}
